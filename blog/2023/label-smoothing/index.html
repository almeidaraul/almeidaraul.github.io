<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Label Smoothing - what it solves and how it works | Raul G. P. de Almeida</title> <meta name="author" content="Raul G. P. de Almeida"> <meta name="description" content="An intuitive explanation of label smoothing"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%AE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://almeidaraul.github.io/blog/2023/label-smoothing/"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Label Smoothing - what it solves and how it works",
      "description": "An intuitive explanation of label smoothing",
      "published": "November 22, 2023",
      "authors": [
        {
          "author": "Raul Almeida",
          "authorURL": "https://almeidaraul.github.io",
          "affiliations": [
            {
              "name": "DInf, UFPR",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="//"><span class="font-weight-bold">Raul </span>G. P. de Almeida</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Label Smoothing - what it solves and how it works</h1> <p>An intuitive explanation of label smoothing</p> </d-title> <d-byline></d-byline> <d-article> <h2 id="introduction">Introduction</h2> <p>There are two explanations here: a short and a long one. I suggest you read both in this order, as the short one might provide an overview of what is going on.</p> <p>This all comes from the original paper by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens and Zbigniew Wojna that proposed label smoothing as a regularization technique<d-cite key="resnet"></d-cite>.</p> <blockquote> <p>It is also important to note I am talking exclusively about classification in this article, so when you read “neural networks are fun” you should actually understand “<strong>classifier</strong> neural networks are fun”.</p> </blockquote> <h2 id="tldr-short-explanation">TL;DR (short explanation)</h2> <p>The general idea of optimizing a neural network’s weights during training is that you want the network’s answers for inputs of different classes to be as distant from one another as possible.</p> <p>Labels are usually represented in one-hot encoded vectors, where one value is equal to 1 and all others are equal to 0, representing the probability of each class (and, since this is ground truth, we know one class has probability 1 and all others have probability 0).</p> <p>Intuitively, the problem is that your model could learn to make the probability of the most likely class to be infinitely greater than the others’ (and understandably so, since 1 is infinitely greater than 0). This leads to a model that doesn’t adapt well yet feels pretty confident about its decisions (☢ overfitting, poor generalization ☢).</p> <p>Label smoothing consists of choosing an Epsilon value and changing the ground- truth labels <code class="language-plaintext highlighter-rouge">y</code> as follows: $y = (1-\epsilon)y + \frac{\epsilon}{|y|}$.</p> <p>With this, nothing is infinitely greater than anything anymore, and your model learns to keep outputs adequate.</p> <h2 id="long-explanation">Long explanation</h2> <p>Now for the long explanation. I will follow the general structure of the authors’ explanation<d-cite key="resnet"></d-cite>, but in my eyes this is less straight to the point in order to make it easier to understand. Just like the authors, I will use the cross-entropy (CE) loss in my explanation.</p> <h3 id="the-problem">The problem</h3> <p>Let’s say you’re optimizing a neural network’s weights by minimizing the CE function over the softmax of its outputs and the expected outputs. There are a couple of things to notice here:</p> <ol> <li>Before the softmax function, your neural network outputs logits, which are unnormalized log-probabilities of each class.</li> <li>Logits are normalized with the softmax function so that your neural network’s predicted probability for class <code class="language-plaintext highlighter-rouge">k</code> is $\textrm{SoftMax}_k = \frac{e^{\textrm{logits}_k}}{\sum_i\log(p_k)\textrm{expected}_k}, i \in \textrm{classes}$</li> <li>With <code class="language-plaintext highlighter-rouge">p_j</code> as the predicted probability of class j and <code class="language-plaintext highlighter-rouge">expected_j</code> as the real probability of the same class, $\textrm{CELoss} = -(\sum_{k\in\textrm{classes}}\log(p_k)\textrm{expected}_k )$</li> </ol> <p>So we can conclude that if you’re minimizing CE, you’re aiming to maximize the log-likelihood of the correct label. <strong>You can’t</strong> maximize this with finite values of <code class="language-plaintext highlighter-rouge">logits[k]</code>. You can, however, get pretty close by making <code class="language-plaintext highlighter-rouge">logits[true class] &gt;&gt; logits[i]</code> for all <code class="language-plaintext highlighter-rouge">i != true class</code>, i.e., by making the ground-truth class logit much greater than all others. If you’ve read the short explanation, this is what I meant by “infinitely greater”.</p> <p>Making the ground-truth class logit much greater than all others leads to two problems:</p> <p><strong>Overfitting.</strong> What your model is learning is to assign full probability to the class it expects to be true, which indicates a very strict learned representation.</p> <p><strong>Little adaptation capability.</strong> It is easy to see at this point that your model is encouraged to output logits so that the largest one is a lot different than all others. What’s important to notice here, is that the gradient $\frac{\partial\textrm{CELoss}}{\partial\textrm{true class logits}}$ is in the range $[-1, 1]$, which reduces the model’s ability to adapt. You can see this as encouraging the model to create very radical outputs and then not being able to make corresponding radical corrections in the optimization step (because the gradient doesn’t explode like the weights do).</p> <p>In summary, these two problems mean your <strong>model is too confident,</strong> which is the final problem that label smoothing solves.</p> <h3 id="the-solution">The solution</h3> <p>Now we need to solve this confidence issue with your model. What the authors<d-cite key="resnet"></d-cite> proposed is to change the ground-truth label $y$ as follows: $y = (1-\epsilon)y + \frac{\epsilon}{|y|}$</p> <p>Or,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y' = (1-epsilon)*y + epsilon/len(y)
</code></pre></div></div> <p>For a chosen <code class="language-plaintext highlighter-rouge">epsilon</code> value. With this change, if the estimated probability of a single class <code class="language-plaintext highlighter-rouge">k</code> gets very close to 1, all others will be very close to 0. Since no probability is “allowed” to actually equal to 0, the softmax output won’t explode, and the computed CE value will be large (i.e., this scenario will be avoided in the optimization process).</p> <p>That’s it! I hope you find this helpful.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-11-22-label-smoothing.bib"></d-bibliography> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Raul G. P. de Almeida. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>