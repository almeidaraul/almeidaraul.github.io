<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://almeidaraul.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://almeidaraul.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-23T01:20:25+00:00</updated><id>https://almeidaraul.github.io/feed.xml</id><title type="html">blank</title><subtitle>Computer Vision Researcher </subtitle><entry><title type="html">Glikoz update</title><link href="https://almeidaraul.github.io/blog/2023/glikoz/" rel="alternate" type="text/html" title="Glikoz update"/><published>2023-11-22T18:30:00+00:00</published><updated>2023-11-22T18:30:00+00:00</updated><id>https://almeidaraul.github.io/blog/2023/glikoz</id><content type="html" xml:base="https://almeidaraul.github.io/blog/2023/glikoz/"><![CDATA[<blockquote> <p>Check out <a href="https://github.com/almeidaraul/glikoz">Glikoz @ GitHub</a></p> </blockquote> <p>I have just finished pushing some small changes to the main glikoz branch, and finally I can call it something I‚Äôd be okay with using for the rest of my life. This opens up space to doing cool things and making things look better, but first I‚Äôll get into some detail about what are the most important features in glikoz, at least for me.</p> <h1 id="what-we-have">What we have</h1> <p>First of all, glikoz works with exports from a very well-established OSS app for recording blood sugar and other relevant information, namely Diaguard. This was very important for me from the beginning, coming from proprietary software that was very restrictive in letting users access their own data in a raw format.</p> <p>With that data, we have all the basic analyses. It is always important for a doctor and their diabetic patients to keep track of how their Time in Range and HbA1c are going. Glikoz reports both the time in range and the estimated HbA1c (you can find references on how the HbA1c is estimated in the GitHub repository). Besides that, the usage of test strips, lancets and fast-acting insulin is reported to help with managing the use of these very expensive resources.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/glikoz1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/glikoz1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/glikoz1-1400.webp"/> <img src="/assets/img/glikoz1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/glikoz2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/glikoz2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/glikoz2-1400.webp"/> <img src="/assets/img/glikoz2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> 24-hour graphs for mean glucose and time-specific time in range. </div> <p>There is also a 24 hour graph with the mean glucose by hour and the reported minimum/maximum as well, and another that presents the Time in Range for each hour instead of all the entries. These are both very important for a less-biased, in-depth review of blood sugar behaviour.</p> <p>For the PDF reports to be useful in actual day-to-day patient data analysis, glikoz also includes a day-by-day table view of the most recent entries. This allows for a close look at what the treatment is looking like on a daily basis.</p> <p>Finally, I added a special page for hypoglycemia-related statistics, reporting the number and percentage of episodes and a graphical view of the distribution of episodes among danger levels. Hypoglycemia is one of the most dangerous aspects of Type 1 Diabetes, and I feel other softwares don‚Äôt go far beyond the simple time-in-range graph. I would say this is one of the most important features in glikoz right now.</p> <h1 id="how-it-was-built">How it was built</h1> <p>I used Python3 for all of glikoz. Since the scope of processing is still very small, and to use it as a proof of concept, Pandas and NumPy are used for data analysis. If you consider that the average user will input about 15 entries every day, we are talking about less than 6 thousand entries (i.e., table rows) in a year, meaning we can allow for even very slow algorithms (think cubic) before starting to worry about execution time.</p> <p>In the future I plan on switching to something distributed, of course, but Pandas will be more than enough in the meantime.</p> <h1 id="plans-for-the-future">Plans for the future</h1> <p>There are, of course, infinite features I want to add to glikoz. One of those that will have to wait a while longer, for example, is reporting analyses with Boukeh, which would allow us to create web-based interactive reports. Another is machine learning-powered analysis to provide insights about the treatment and optimization of insulin dosage parameters based on recent data.</p> <p>The two most important things, though, and these are the ones I‚Äôll be working on next, are documentation and libre integration.</p> <p>I made sure to write readable code with inline documentation (and since a while ago I‚Äôve been doing test-driven development, which helps with that), but I feel glikoz still lacks a textual explanation of its purpose, architecture and backlog. This is not a coding task, which might be the reason for me not having done it yet, but still it‚Äôs very important.</p> <p>And the Freestyle Libre integration is probably the most useful thing I can do to access more data. This will allow for minute-by-minute analysis of blood sugar behavior. I‚Äôm really looking forward to it. Since this means a lot more data, however, I plan on moving to SQL-powered analysis wherever possible. This will also help in scaling later on.</p> <h1 id="final-remarks">Final remarks</h1> <p>Well, that was it. I truly am excited to continue working on glikoz, and I hope I can learn a lot from these next steps.</p>]]></content><author><name></name></author><category term="python"/><summary type="html"><![CDATA[We officially have a decent version! And plans for the future]]></summary></entry><entry><title type="html">Label Smoothing - what it solves and how it works</title><link href="https://almeidaraul.github.io/blog/2023/label-smoothing/" rel="alternate" type="text/html" title="Label Smoothing - what it solves and how it works"/><published>2023-11-22T18:25:00+00:00</published><updated>2023-11-22T18:25:00+00:00</updated><id>https://almeidaraul.github.io/blog/2023/label-smoothing</id><content type="html" xml:base="https://almeidaraul.github.io/blog/2023/label-smoothing/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>There are two explanations here: a short and a long one. I suggest you read both in this order, as the short one might provide an overview of what is going on.</p> <p>This all comes from the original paper by Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens and Zbigniew Wojna that proposed label smoothing as a regularization technique<d-cite key="resnet"></d-cite>.</p> <blockquote> <p>It is also important to note I am talking exclusively about classification in this article, so when you read ‚Äúneural networks are fun‚Äù you should actually understand ‚Äú<strong>classifier</strong> neural networks are fun‚Äù.</p> </blockquote> <h2 id="tldr-short-explanation">TL;DR (short explanation)</h2> <p>The general idea of optimizing a neural network‚Äôs weights during training is that you want the network‚Äôs answers for inputs of different classes to be as distant from one another as possible.</p> <p>Labels are usually represented in one-hot encoded vectors, where one value is equal to 1 and all others are equal to 0, representing the probability of each class (and, since this is ground truth, we know one class has probability 1 and all others have probability 0).</p> <p>Intuitively, the problem is that your model could learn to make the probability of the most likely class to be infinitely greater than the others‚Äô (and understandably so, since 1 is infinitely greater than 0). This leads to a model that doesn‚Äôt adapt well yet feels pretty confident about its decisions (‚ò¢ overfitting, poor generalization ‚ò¢).</p> <p>Label smoothing consists of choosing an Epsilon value and changing the ground- truth labels <code class="language-plaintext highlighter-rouge">y</code> as follows: $y = (1-\epsilon)y + \frac{\epsilon}{|y|}$.</p> <p>With this, nothing is infinitely greater than anything anymore, and your model learns to keep outputs adequate.</p> <h2 id="long-explanation">Long explanation</h2> <p>Now for the long explanation. I will follow the general structure of the authors‚Äô explanation<d-cite key="resnet"></d-cite>, but in my eyes this is less straight to the point in order to make it easier to understand. Just like the authors, I will use the cross-entropy (CE) loss in my explanation.</p> <h3 id="the-problem">The problem</h3> <p>Let‚Äôs say you‚Äôre optimizing a neural network‚Äôs weights by minimizing the CE function over the softmax of its outputs and the expected outputs. There are a couple of things to notice here:</p> <ol> <li>Before the softmax function, your neural network outputs logits, which are unnormalized log-probabilities of each class.</li> <li>Logits are normalized with the softmax function so that your neural network‚Äôs predicted probability for class <code class="language-plaintext highlighter-rouge">k</code> is $\textrm{SoftMax}_k = \frac{e^{\textrm{logits}_k}}{\sum_i\log(p_k)\textrm{expected}_k}, i \in \textrm{classes}$</li> <li>With <code class="language-plaintext highlighter-rouge">p_j</code> as the predicted probability of class j and <code class="language-plaintext highlighter-rouge">expected_j</code> as the real probability of the same class, $\textrm{CELoss} = -(\sum_{k\in\textrm{classes}}\log(p_k)\textrm{expected}_k )$</li> </ol> <p>So we can conclude that if you‚Äôre minimizing CE, you‚Äôre aiming to maximize the log-likelihood of the correct label. <strong>You can‚Äôt maximize this with finite values of <code class="language-plaintext highlighter-rouge">logits[k]</code></strong>. You can, however, get pretty close by making <code class="language-plaintext highlighter-rouge">logits[true class] &gt;&gt; logits[i]</code> for all <code class="language-plaintext highlighter-rouge">i != true class</code>, i.e., by making the ground-truth class logit much greater than all others. If you‚Äôve read the short explanation, this is what I meant by ‚Äúinfinitely greater‚Äù.</p> <p>Making the ground-truth class logit much greater than all others leads to two problems:</p> <p><strong>Overfitting.</strong> What your model is learning is to assign full probability to the class it expects to be true, which indicates a very strict learned representation.</p> <p><strong>Little adaptation capability.</strong> It is easy to see at this point that your model is encouraged to output logits so that the largest one is a lot different than all others. What‚Äôs important to notice here, is that the gradient $\frac{\partial\textrm{CELoss}}{\partial\textrm{true class logits}}$ is in the range <code class="language-plaintext highlighter-rouge">[-1, 1]</code>, which reduces the model‚Äôs ability to adapt. You can see this as encouraging the model to create very radical outputs and then not being able to make corresponding radical corrections in the optimization step (because the gradient doesn‚Äôt explode like the weights do).</p> <p>In summary, these two problems mean your <strong>model is too confident,</strong> which is the final problem that label smoothing solves.</p> <h3 id="the-solution">The solution</h3> <p>Now we need to solve this confidence issue with your model. What the authors<d-cite key="resnet"></d-cite> proposed is to change the ground-truth label <code class="language-plaintext highlighter-rouge">y</code> as follows: $y = (1-\epsilon)y + \frac{\epsilon}{|y|}$</p> <p>Or,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y' = (1-epsilon)*y + epsilon/len(y)
</code></pre></div></div> <p>For a chosen <code class="language-plaintext highlighter-rouge">epsilon</code> value. With this change, if the estimated probability of a single class <code class="language-plaintext highlighter-rouge">k</code> gets very close to 1, all others will be very close to 0. Since no probability is ‚Äúallowed‚Äù to actually equal to 0, the softmax output won‚Äôt explode, and the computed CE value will be large (i.e., this scenario will be avoided in the optimization process).</p> <p>That‚Äôs it! I hope you find this helpful.</p>]]></content><author><name>Raul Almeida</name></author><category term="machine_learning"/><summary type="html"><![CDATA[An intuitive explanation of label smoothing]]></summary></entry><entry><title type="html">Making sense of PyTest imports</title><link href="https://almeidaraul.github.io/blog/2023/pytest-imports/" rel="alternate" type="text/html" title="Making sense of PyTest imports"/><published>2023-11-22T18:10:00+00:00</published><updated>2023-11-22T18:10:00+00:00</updated><id>https://almeidaraul.github.io/blog/2023/pytest-imports</id><content type="html" xml:base="https://almeidaraul.github.io/blog/2023/pytest-imports/"><![CDATA[<p>Python imports can get weird, specially when dealing with tests that are to be kept separate from the application code (i.e., in sibling directories).</p> <p>This is a quick guide on how you can organize your tests with <code class="language-plaintext highlighter-rouge">pytest</code> so that everything works as expected. If <code class="language-plaintext highlighter-rouge">pytest</code> is new for you, check out their <a href="https://docs.pytest.org/en/">docs</a>. PyTest is a framework for writing small, readable tests for python programs.</p> <p>I‚Äôve kept this article as a note to myself and friends, and now I am publishing it online. It is made to be brief, with minimal explanations, because it shows you an example of how to do something pretty simple.</p> <p>Do note that while this is not the only way to make things work, it‚Äôs my favorite out of everything I‚Äôve come across before.</p> <h1 id="organizing-pytest-tests">Organizing <code class="language-plaintext highlighter-rouge">pytest</code> tests</h1> <h2 id="the-code">The code</h2> <p>Your code should be organized in packages, and the directory with all the tests should be right next to the package directories.</p> <p>This might sound counterintuitive because the test code from the <code class="language-plaintext highlighter-rouge">tests</code> directory should not be able to reach your package, since it is not in a child directory. The reason this works, in a nutshell, is that <code class="language-plaintext highlighter-rouge">pytest</code> operates on a broader scope.</p> <h3 id="directory-structure">Directory structure</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pkg/
  code.py
tests/
  __init__.py
  test_code.py
</code></pre></div></div> <h3 id="file-structure">File structure</h3> <p>As for your files, you can basically use imports as you would feel the most comfortable doing: from each package you can import a module, or something from this module.</p> <p>The <code class="language-plaintext highlighter-rouge">tests</code> package needs an <code class="language-plaintext highlighter-rouge">__init__</code> file.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># pkg/code.py
</span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">+</span><span class="mi">2</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># tests/__init__.py
# (may be left empty)
</span></code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># tests/test_code.py
</span><span class="kn">from</span> <span class="n">pkg.code</span> <span class="kn">import</span> <span class="n">f</span>


<span class="k">def</span> <span class="nf">test_f</span><span class="p">():</span>
    <span class="k">assert</span> <span class="nf">f</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>
</code></pre></div></div> <h1 id="the-testing">The testing</h1> <p>Now you can run your tests with the <code class="language-plaintext highlighter-rouge">pytest</code> command. That‚Äôs it! üòä</p>]]></content><author><name></name></author><category term="python"/><summary type="html"><![CDATA[Importing things from sibling directories]]></summary></entry><entry><title type="html">posts to do</title><link href="https://almeidaraul.github.io/blog/2023/post-bibliography/" rel="alternate" type="text/html" title="posts to do"/><published>2023-08-27T22:30:00+00:00</published><updated>2023-08-27T22:30:00+00:00</updated><id>https://almeidaraul.github.io/blog/2023/post-bibliography</id><content type="html" xml:base="https://almeidaraul.github.io/blog/2023/post-bibliography/"><![CDATA[<p>Posts that I plan on writing/posting here (for those already written):</p> <ul> <li>Explanation of the SemEval paper</li> <li>Making sense of PyTest imports</li> <li>Finding/reading AI papers</li> <li>Label smoothing, explained</li> <li>Early stopping aux (as a post explaining early stopping)</li> <li>Please use markdown</li> <li>something about icpc br reg 2023 (campo grande)</li> </ul>]]></content><author><name></name></author><category term="todo"/><summary type="html"><![CDATA[posts that I plan on writing]]></summary></entry></feed>